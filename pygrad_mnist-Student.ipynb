{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying MNIST with Le-Net (MyGrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be training a convolutional neural network (using the Le-Net design described in this paper: http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) to classify hand-written digits. We will be using the MNIST dataset, which contains labeled images of hand-written digits from 0 to 9 (http://yann.lecun.com/exdb/mnist/). The MNIST dataset has a training set of 60,000 images and a test set of 10,000 images. \n",
    "\n",
    "We will be using LeNet architecture, which was one of the first convolutional neural network designs. We will explain the architecture and operations used in convolutional neural nets throughout this notebook. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mygrad import Tensor\n",
    "from mygrad.nnet.layers import *\n",
    "from mygrad.nnet.activations import *\n",
    "try:\n",
    "    from jupyterthemes import jtplot\n",
    "    jtplot.style()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to write a few functions that will help us with loading and processing data. To load and process our MNIST data, we will need functions to first load he data as a tuple of the training and testing data and labels, a function to create a normal distribution centered at 0 with a variance calculated using He initialization, a function to create a uniform distribution centered at 0 with variance calculated with the Xavier initialization, and a function (much like ones we've written before) to measure the accuracy of our predictions in comparison to the actual labels of the testing set. \n",
    "\n",
    "If you would like to read more about how Xavier Glorot explains the rationalization behind these weight initializations, look here for his paper written with Yoshua Bengio. (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_numpy_data(fname, entries=('x_train', 'y_train', 'x_test', 'y_test')):\n",
    "    '''\n",
    "    Arguments: \n",
    "    fname: the file from which you are loading data\n",
    "    entries: the typical set of labeled training data and labeled test data \n",
    "    \n",
    "    Functionality: \n",
    "    Process the data in fname as a tuple of all data.\n",
    "    '''\n",
    "\n",
    "\n",
    "def he_normal(shape, gain):\n",
    "    '''\n",
    "    Arguments:\n",
    "    shape: tuple, dimensions of layer\n",
    "    gain: the scalar by which you will scale the variance.\n",
    "    \n",
    "    Functionality: \n",
    "    Create a normal distribution centered at 0 with a variance calculated with He initialization.\n",
    "    \n",
    "    He weight initialization is initializing a normal distribution with a standard deviation of sigma=gain*sqrt{1/fan_in}\n",
    "    Recall also that to calculate fan_in, you simply multiply fan_in = n_feature_maps_in * receptive_field_height * receptive_field_width\n",
    "    '''\n",
    "\n",
    "\n",
    "def xavier_uniform(shape, gain):\n",
    "    '''\n",
    "    Arguments:\n",
    "    shape: tuple, dimensions of layer\n",
    "    gain: the scalar by which you will scale the variance.\n",
    "    \n",
    "    Functionality:\n",
    "    Create a uniform distribution centered at 0 with a variance calculated with Xavier initialization. \n",
    "    Xavier initialization of a uniform distribution is scaled such that the standard deviation is \n",
    "    sigma = gain*sqrt{6/fan_in}\n",
    "    '''\n",
    "\n",
    "\n",
    "def measure_acc(ypred, ytrain):\n",
    "    '''\n",
    "    Arguments: \n",
    "    ypred, ytrain - your predicted labels and your actual labels from your training set. \n",
    "    \n",
    "    Functionality:\n",
    "    Much like in previous notebooks, calculate the accuracy of your predictions based on existing labels\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load your data from mnist.npz using your load_numpy_data function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Layers of our ConvNet\n",
    "\n",
    "Now, we will construct the main architecture for our convolutional neural network. \n",
    "\n",
    "The main difference between the neural networks we have previously discussed and a convolutional neural network is that every layer in a CNN maps a 3-D input to a 3-D output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![check out this diagram](cnn_diagram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does this work? There are three types of layers used in convnets--convolutional layers, pooling layers, and dense layers. \n",
    "\n",
    "In CNN's, convolutions are meant to preserve the spatial relationships between pixels in the input image. Essentially, a filter is convolved upon the image, (see here for a nice gif, just ctrl+F \"convolution demo\" http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "By sliding the filter matrix across the entire image and calculating the dot product at each position, we create the feature map, also known as a convolved feature or an activation map. Different filter matrices will create different results. \n",
    "\n",
    "![check out this diagram 2](matrices.png)\n",
    "\n",
    "Then, the dimensions of the feature map are defined by depth (the number of filters applied in this convolution process), stride (the number of pixels by which we shift our filter everytime we shift to a new position), and zero-padding (adding a border of zeros around our image to allow for a \"wide convolution.\" A image without this border of zeros, without the zero-padding, would be a \"narrow convolution.\"\n",
    "\n",
    "Another type of operation is ReLU. the ReLU operation is basically another filter that we can apply to an image, but rather than accepting negative values, if there are any negative values we just set it to 0. ReLU stands for Rectified Linear Unit and is a non-linear function. Below is an example of the output of applying the ReLU filter, which is applied pixel-wise. \n",
    "\n",
    "![check out this diagram 3](ReLU.png)\n",
    "\n",
    "Another kind of layer is the pooling layer. Spatial Pooling (aka subsampling or downsampling) essentially divides up our input into different windows, and applies an operation to each window, such as max pooling, average pooling, or sum pooling. Essentilly, this just downsizes the image into a map that takes either the max, average, or sum of all values in each segment and maps that to the output. \n",
    "\n",
    "![check out this diagram 3](pooling1.jpg)\n",
    "![check out this diagram 3](poolingimage2.jpg)\n",
    "\n",
    "We will only be using max-pooling in this notebook, as max-pooling is usually the most effective. Note that although we are downsizing our input images by passing our inputs through a pooling layer, we apply this pooling operation separately to each feature map that is produced through our convolutional layer. Pooling makes the general amount of data we are processing more manageable, prevents overfitting by reducing the number of parameters, and minimizes the effects of small distortions in the input image (as the effect on the maximum/average will be trivial). \n",
    "\n",
    "Next, we also have fully-connected (FC), aka dense layers. The dense layer is like a multi-layer perceptron, and essentially generates a probability distribution across the classes, such that the generated probabilities sum to 1. Our dense layer will take the high level, distilled features that are output from the convolutional and pooling layers to process and classify the image. The dense layer is essentially what we have already done before in class. For this network, we will be using a softmax activation function to generate our output. \n",
    "\n",
    "In the convnet to classify MNIST images, we will construct a CNN with two convolutional layers (each paired with a pooling layer), one ReLU layer, and one final output layer with the softmax activation function. \n",
    "\n",
    "To begin, initialize the tuples representing input/output dimensions, the dimensions of our layers, stride/pool sizes, and other constants that may be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize two variables representing the input size and the output dimensions. Keep in mind the goal of our classifier,\n",
    "#and that the size of the images will be 32x32 (as usual)\n",
    "#STUDENT CODE HERE:\n",
    "input_size =\n",
    "output_dims = \n",
    "\n",
    "# Next, define the model parameters by initializing the dropout probability as a float, and the convolutional strides\n",
    "# and pooling shape and stride. Just for good measure, try a dropout probability of 0.5.\n",
    "\n",
    "\n",
    "\n",
    "# dropout probability (i.e., percentage of elements to randomly drop)\n",
    "# convolutional strides\n",
    "# pooling shape and stride\n",
    "\n",
    "\n",
    "#Initialize variables representing the shape of our first and second layers, which are both convolutional, as well as \n",
    "#our third and fourth layers which will both be dense layers. Remember that the dimensisons of the layers will change \n",
    "#throughout the network. \n",
    "\n",
    "\n",
    "\n",
    "#Now, initialize the W and b parameters for our four layers using helper functions you have previously written\n",
    "#for creating properly scaled probability distributions. \n",
    "\n",
    "\n",
    "\n",
    "#Here, create a list of the weights and biases you just initialized above and also feel free to define any \n",
    "#constants that you will refer to later for good coding practice and to make your code more readable. \n",
    "#Be sure to initialize arrays in which to store loss and accuracy as you will be graphing these later on.\n",
    "params =\n",
    "reg = \n",
    "lr =\n",
    "mu = \n",
    "p = \n",
    "vs = \n",
    "l = \n",
    "acc = \n",
    "N ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train your classfier for 200 iterations. It should take about 6-7 minutes to run. Be sure to record loss and accuracy just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.math import log\n",
    "from numpy.random import rand\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph your loss and accuracy! These should be familiar shapes. Old friends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've trained and tested your first convolutional neural network! If you're feeling motivated, try to create a function that will define a conv net model just like the one you've just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
